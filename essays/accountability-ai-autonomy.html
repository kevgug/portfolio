<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" sizes="180x180" href="../touch-icon.png" />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="../favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="../favicon-16x16.png"
    />
    <link rel="manifest" href="../site.webmanifest" />
    <meta name="viewport" content="width=device-width" />
    <link
      rel="alternate"
      type="application/rss+xml"
      title="Kevin Gugelmann's Essays"
      href="/rss.xml"
    />
    <meta http-equiv="content-security-policy" content="">
		<link href="../internal/immutable/assets/_layout-29a140e3.css" rel="stylesheet">
		<link href="../internal/immutable/assets/index-d573e953.css" rel="stylesheet">
		<link href="../internal/immutable/assets/Footer-1c581a53.css" rel="stylesheet">
		<link href="../internal/immutable/assets/_layout-691a1879.css" rel="stylesheet">
		<link href="../internal/immutable/assets/_page-2e16a816.css" rel="stylesheet">
		<link rel="modulepreload" href="../internal/immutable/start-9a263591.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/index-1b60df3c.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/singletons-d646bf5e.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/index-9ebf557e.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/control-e7f5239e.js">
		<link rel="modulepreload" href="../internal/immutable/components/pages/_layout.svelte-ecd7fa6c.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/index-67682206.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/stores-de652803.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/ScrollTrigger-9ff9d99e.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/reliableScroll-d53edb5e.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/index-cf87e9ff.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/Image-892e2a7f.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/Icon-5f1d7f92.js">
		<link rel="modulepreload" href="../internal/immutable/modules/pages/_layout.ts-9cbb603b.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/_layout-da46b06b.js">
		<link rel="modulepreload" href="../internal/immutable/components/pages/essays/_layout.svelte-936fa4b0.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/Footer-9f09e909.js">
		<link rel="modulepreload" href="../internal/immutable/components/pages/essays/_slug_/_layout.svelte-e3ff3b01.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/essayNav-a8b4dd3d.js">
		<link rel="modulepreload" href="../internal/immutable/components/pages/essays/_slug_/_page.svelte-a6161756.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/marked-13f94b6b.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/Separator-e4a3d9e5.js">
		<link rel="modulepreload" href="../internal/immutable/modules/pages/essays/_slug_/_page.ts-92898bec.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/_page-8dfc6889.js">
		<link rel="modulepreload" href="../internal/immutable/chunks/index-b40a4f3c.js"><title>Selling software without accountability</title><!-- HEAD_svelte-iu3vwn_START --><!-- HEAD_svelte-iu3vwn_END -->
  </head>
  <body data-sveltekit-preload-data="hover">
    <div style="display: contents">


<div class="invisible pointer-events-none" aria-hidden="true"></div>

<div class="contents"><nav class="w-full fixed top-0 z-50 bg-transparent pointer-events-none">
  <div class="absolute inset-x-0 top-0 h-28 md:h-32 bg-gradient-to-b from-background via-background/80 to-transparent pointer-events-none"></div>
  <div class="relative z-10 flex flex-row justify-between items-center mx-auto w-full max-w-screen-2xl px-5 md:px-[2.5rem] xl:px-[5rem] h-16 md:h-20">
    <div class="flex flex-row items-center space-x-2 md:space-x-3 pointer-events-auto"><a href="/essays" aria-label="Back to essays" class="p-2 -ml-2"><span style=""><svg class="text-white" style="overflow: visible;" width="1em" height="1em" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path d="M23.25 12 0.75 12" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></path><path d="M11.25 1.5 0.75 12l10.5 10.5" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span></a></div>

    
    <div class="flex items-center pointer-events-auto"><div class="dropdown-root is-subtle svelte-4df8jd"><button type="button" class="dropdown-trigger svelte-4df8jd" aria-haspopup="listbox" aria-expanded="false"><span class="dropdown-label svelte-4df8jd">Sections</span>
    <span class="chevron-wrapper svelte-4df8jd"><svg width="18px" height="18px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" class="chevron-svg svelte-9q6psm" aria-hidden="true"><path d="M 7 10 L 12 15 L 17 10" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" vector-effect="non-scaling-stroke"></path></svg></span></button>

  
</div></div></div></nav>

<div class="flex flex-col mx-auto w-screen max-w-screen-2xl px-5 md:px-[2.5rem] xl:px-[5rem] pt-20 md:pt-24 space-y-8">

<article class="py-8 md:py-12 w-full svelte-1hr1h2"><div><header class="max-w-screen-md mx-auto"><h1 class="text-3xl md:text-5xl font-bold text-glacial-blue leading-[1.175] md:leading-[1.1]">Selling software without accountability</h1>
      <div class="flex items-center gap-2 mt-3.5 md:mt-4"><p class="text-muted-text-grey">October 2025</p>
        </div></header>
    <div class="my-12 md:my-14 max-w-screen-md mx-auto"><div class="w-full h-[1px] rounded-sm bg-separator-grey"></div></div>
    <div class="space-y-12 md:space-y-14"><section id="section-0" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">The question</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-1"><span><!-- HTML_TAG_START -->When my brother visited me in Chicago to run the 47th Bank of America marathon (on virtually no training!), he raised an interesting question: is <strong>accountability</strong> a good enough reason for why we don&#39;t want autonomous AI software engineers? We both agree that full autonomy is not even feasible for at least another two years. The question is more about <em>when</em> AI autonomy becomes feasible, will the need for human accountability get in the way? <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">1</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->If accountability is necessary for running a profitable business, it follows that humans should always have oversight over every product decision and feature implementation; at most, AI would be a coworker, but could never fully displace humans. Alternatively, accountability is not an important part of building software products. In that scenario, AI can successfully decide what to build and then presumably build it too; software engineers and product designers could very well be replaced by fully autonomous AI.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->I view accountability as the idea that someone owns a product decision and accepts whatever consequences may follow, good or bad. If we let AI autonomously decide what to do and how to implement those changes, then we can&#39;t say that any person truly <em>owns</em> those decisions. At best, we can say that someone instructed the AI on how to operate at a high level, which led to some product outcome. But that&#39;s not a generalizable argumentâ€”sometimes the decision is a direct result of the human&#39;s system instructions, while other times the AI has gone rogue. In any case, autonomous AI prevents clear accountability with any person.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->So maybe we can&#39;t pinpoint <em>exactly</em> who or what is responsible for certain features, bugs, or errors. Does that negatively affect the bottom line of the software company? To rephrase my brother&#39;s question:<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="w-full max-w-screen-md mx-auto"><div class="-mx-[calc(1rem+4px)] md:-mx-[calc(1.5rem+4px)]"><blockquote class="px-4 md:px-6 border-l-4 md:rounded-r-lg bg-white/5 border-white svelte-fxv321 !my-7 md:!my-9       py-3.5 md:py-4"><p class="font-serif text-white font-semibold text-3xl md:text-4xl leading-tight md:leading-snug"><!-- HTML_TAG_START -->How important is accountability in product teams for building software that sells?<!-- HTML_TAG_END --></p>
</blockquote></div>
            </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Let&#39;s start by looking at our assumptions about why we might need accountability in software teams, meaning each product decision can be traced back to an individual employee.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section><section id="section-1" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Benefits of accountability</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->I see two main arguments for why firms need accountability to sell software in the pre-AI age.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->One is <strong>competitive pressure</strong> from inside the firm. If a knowledge worker makes exceptionally good work they expect promotion, while for exceptionally poor work they can expect to be fired. Accountability motivates employees to do good work and avoid bad work, because everyone owns their decisions and any subsequent consequences. This motivation produces better software products, which helps create a competitive edge in the software market.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Another is the <strong>customer trust</strong>. Naturally, the customer wants to know that the software creator won&#39;t betray them. If product teams can make decisions without consequences, what stops them from turning their back on the user? With accountability in place, individual employees are directly responsible for lost customers and revenue if they sunset important features or expose publicly-identifiable information (PII).<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-2"><span><!-- HTML_TAG_START -->Both benefits of accountability help sell software. Are these benefits exclusive to accountability? Let&#39;s consider three realistic scenarios in which AI builds software on its own. <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">2</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div>
        </section><section id="section-2" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Autonomy scenarios</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Imagine AI automation moves up the chain, first replacing engineers with designers who direct autonomous AI to make engineering decisions, then replacing designers with product managers who direct autonomous AI to make design decisions. Even the most technical product managers are swapped with C-suite directing increasingly intelligent yet holistic AI systems. Before you know it, even C-suite is offloading all their decisions. At this point, AI is hypothetically making decisions across every facet of the company. There is not a single employee who can be held accountable for any specific feature, bug, or system failure. Who is accountable when something goes wrong?<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-3"><span><!-- HTML_TAG_START -->There are three plausible scenarios in which all software is designed, coded, and deployed exclusively by AI. The first is exactly as I described: some <strong>small-team company</strong> where the C-suite instructs AI broadly on the company&#39;s mission and guiding principles, before kicking their feet up and letting AI make all the product decisions. The second scenario eliminates the company entirely, which is now just a middleman between the user and AI building the softwareâ€”the <strong>user gives AI high-level instructions</strong> on what software they want built for themselves. In the third scenario, the app builder platform uses context about the user to <strong>anticipate the user&#39;s needs</strong>, then builds the software accordingly. <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">3</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->In each scenario, no one can be held fully accountable for how the software turns out, neither the user nor the creator. Since humans can only give AI a finite set of instructions, its decisions will mostly be extrapolations of human rules and intent.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->I&#39;d like to test if only accountability can achieve competitive pressure and customer trust, assuming they are both necessary for selling software products. Then we can infer if the absence of accountability, when using autonomous AI systems, would necessarily harm profits.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section><section id="section-3" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Alignment</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Alignment is a notable issue with autonomous systems. Even if AI follows all the provided instructions to a T, there will always be edge cases that humans couldn&#39;t anticipate or encode preferences for. The system is fully autonomous, so it can&#39;t ever ask the human to state or clarify their preferences along the way. AI will need to fill in the gaps with its own preferences, which may or may not be what the software creator would have wanted. There is either human-AI alignment or misalignment: it could go either way.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Let&#39;s assume AI makes a decision that is misaligned. The product worsens, users churn, and revenue drops. There is no clear accountability: no individual human made the problematic decision, yet the software suffers. We should ask whether the lack of clear accountability actually hurts the business. Importantly, the market doesn&#39;t care <em>who</em> made a bad callâ€”it will respond the same to a misaligned AI as it would to a human making the same misstep. While there is no competitive pressure at an individual level, there is competitive pressure at a company level. If things go sideways, the autonomous system can pick up on that signal and act accordingly.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Autonomous AI might also make an aligned product decision. The product improves and revenue grows. Again, there is no clear accountability: we can&#39;t trace the beneficial decision back to an individual. As before, the market doesn&#39;t care exactly who created that feature or fixed that bug. So customer trust improves even though accountability is absent. Competitive pressure also acts on the company as a wholeâ€”the AI systemâ€”rather than on individuals that need incentives to work well.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->So there is still competitive pressure even when there&#39;s no accountability; the feedback mechanism just acts on a company level rather than an individual level. When aligned, autonomous AI can still maintain or improve customer trust. It&#39;s just uncertain whether software changes are aligned (beneficial) or misaligned (harmful), because the human&#39;s instructions will have gaps that AI needs to fill on its own.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->All in all, autonomous AI can achieve the same main benefits as accountabilityâ€”competitive pressure and customer trustâ€”just by different, and arguably less certain, means. So autonomous AI can be financially viable, too.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section><section id="section-4" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Blaming users</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->But what about our intuition that someone, <em>anyone</em>, should be responsible when software goes wrong? We can look back at each scenario for creating software with autonomous AI and point fingers at whoever wrote the AI instructions. For commercial software, we might blame the C-suite. For prompt-based personal software, we might blame the user. And for personal software that anticipates user needs, we might blame the app builder platform. Then again, our previous conclusion hasn&#39;t changed: no one can be held fully accountable for how the software turns out. We&#39;re just running in circles.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Rather than chasing accountability with the software creators, what if we can instead shift the responsibility of good software to the user? This seems backwards and appears to violate every UX principle. The reason is that software markets in the AI age will be highly saturated, due to the rapidly falling barriers for creating softwareâ€”both skills and cost.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="w-full max-w-screen-md mx-auto"><div class="-mx-[calc(1rem+4px)] md:-mx-[calc(1.5rem+4px)]"><blockquote class="px-4 md:px-6 border-l-4 md:rounded-r-lg bg-white/5 border-white svelte-fxv321 !my-7 md:!my-9       py-3.5 md:py-4"><p class="font-serif text-white font-semibold text-3xl md:text-4xl leading-tight md:leading-snug"><!-- HTML_TAG_START -->It&#39;s not unlikely that a user can choose from thousands of close software substitutes in the near future.<!-- HTML_TAG_END --></p>
</blockquote></div>
            </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Accountability will disappear as human teams disappear, and the main responsibility of having a good user experience will fall on the users. Because users have many options, they have greater responsibility over which software product they decide to use.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section><section id="section-5" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Blind trust</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->You may reasonably say, &quot;but that doesn&#39;t solve the issue of information asymmetry!&quot; I completely agree. Let&#39;s tackle this separate issue.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-4"><span><!-- HTML_TAG_START -->Users cannot definitively know ahead of using software if it will work as advertised. However, even today when there are thousands of close substitutes programs for every use case, the overwhelming majority of users blindly accept terms &amp; conditions (T&amp;Cs) and end user license agreements (EULAs). In other words, 99.99% of consumers are already comfortable with blindly trusting the goodwill of software creators to write fine print that is sufficiently aligned with their own goals. After all, EULAs have become longer and more complex over the last decade, and there has only been an increase in software usage. <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">4</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-5"><span><!-- HTML_TAG_START -->I see a parallel in blind trust between human-written and AI-written software. I believe that increased AI use in software production will, ceteris paribus, have no effect on software adoption. Most major companies are already writing their software using AI, representing a nonzero percentage of their code. I&#39;m aware some modern companies, like Ramp, even demand their engineers use AI tools to stay ahead of the curve. The software engineering industry has already dialed up the percentage of code produced by AI and the adoption of software has not been affected at all, at least not to my knowledge. If thatâ€™s our analysis at the margin, we also won&#39;t expect any difference in adoption between software that is 0%, 50%, 90%, 99.9%, or 100% AI made. 100% percentage represents, of course, our autonomous AI system. Why is that? <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">5</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->I think this effect is for pragmatic reasonsâ€”all customers should take a product at face value to a certain degree. Consider these reasonable user questions:<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><ul class="list-disc pl-5 space-y-2 font-serif text-description-text-grey my-6"><li><span class="li-content"><!-- HTML_TAG_START -->Could my phone battery blow up in my hand?<!-- HTML_TAG_END --></span></li><li><span class="li-content"><!-- HTML_TAG_START -->Could the tires on my car fall off on the motorway?<!-- HTML_TAG_END --></span></li><li><span class="li-content"><!-- HTML_TAG_START -->Could the roof in my lecture hall fall on my head mid-class?<!-- HTML_TAG_END --></span></li>
                    </ul>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->You could compile a list of valid concerns about using <em>any</em> product, if you put your mind to it. Even something as harmless as a poster on your wall risks you leaving a bad impression on someone important you invite over. The reason we donâ€™t obsess over such low risks is because we simply want the product that offers maximum upside while falling under a risk thresholdâ€”both decided jointly by logical analysis and our gut feeling. The same applies to software products:<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="w-full max-w-screen-md mx-auto"><div class="-mx-[calc(1rem+4px)] md:-mx-[calc(1.5rem+4px)]"><blockquote class="px-4 md:px-6 border-l-4 md:rounded-r-lg bg-white/5 border-white svelte-fxv321 !my-7 md:!my-9       py-3.5 md:py-4"><p class="font-serif text-white font-semibold text-3xl md:text-4xl leading-tight md:leading-snug"><!-- HTML_TAG_START -->We can only know if software works as expected by using it.<!-- HTML_TAG_END --></p>
</blockquote></div>
            </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-6"><span><!-- HTML_TAG_START -->As for trusting the <em>legal</em> goodwill of software creators, you might think we largely trust T&amp;Cs and EULAs because we assume at least one other user <em>does</em> read T&amp;Cs and EULAs and would publicly flag any harmful fine print. Firstly, this is a precarious assumption: not everyone is an Edward Snowdon who finds and reports harmful software practices, and not every software program has been subjected to an Edward Snowdon. Especially in the AI age when there are thousands of close substitutes. Secondly, I would expect an AI system to be far more reliable, accurate, and faster at detecting harmful fine print. <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">6</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->In other words, I don&#39;t think the majority of users blindly accept EULAs because they are all free riding on some legal nerd who will read them. No, we just all accept that we can&#39;t know for sure if software will work as advertised or breach our trust. We&#39;re simply practical creatures: we know the fastest way to find out if software works is to use it, and the fastest way to use software is to skip reading the legal paperwork.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section><section id="section-6" data-essay-section="true"><div class="max-w-screen-md mx-auto flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Conclusion</h2>
            </div>
          <div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->It turns out, accountability is <em>not</em> vital for selling software. The main benefits of accountabilityâ€”competitive pressure and customer trustâ€”can also be achieved by autonomous AI systems. Competitive pressure is simply shifted from the individual who needs status-based motivation to the company which continuously collects signalsâ€”the company being an autonomous AI system. Customer trust is still feasible, but harder to guarantee when there is no human to validate each decision.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey" id="footnote-ref-7"><span><!-- HTML_TAG_START -->The customer doesn&#39;t need perfect trust in a software product to start using it; it just needs to be <em>promising enough</em>. At the same time, if autonomous AI is misaligned with what the creator would have wanted and neglects a user, they will still eventually offboard the software, factoring in switching costs. This is analogous to how users blindly accept T&amp;Cs and EULAs for modern software, proving that the vast majority of users accept information asymmetry to a degree and simply look for sufficiently promising products. <!-- HTML_TAG_END -->
      </span><button class="group inline-flex items-baseline border-none bg-transparent p-0 cursor-pointer"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.32rem]"> </span>
          <span class="underline decoration-glacial-blue/60 group-hover:decoration-glacial-blue">7</span><span class="text-[0.32rem]"> </span>]
        </span>
      </button></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Counter to our intuition, users will be responsible for using bad software in the AI age. The falling cost and skills barrier for building software will flood the commercial software market with thousands of close substitutes. Alternatively, users will build their own hyperpersonalized software with AI app builders, meaning they effectively input their needs and also bear responsibility for any poor UX.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Across thousands of commercial and personal software products doing generally the same thing, there will be at least one high quality option. The main barriers to picking high quality software will be switching costs, which is already relevant today, and evaluating all substitutes well, which is a future problem. To solve finding the best option, I expect someone will build a QA testing tool for <em>users</em> that tests software on their behalf before they onboard the best one, which is deserving of its own essay.<!-- HTML_TAG_END -->
      </span></div>
                </div><div class="space-y-4 max-w-screen-md mx-auto mt-3"><div class="font-serif text-description-text-grey"><span><!-- HTML_TAG_START -->Human-AI alignment remains the biggest limiting factor in letting AI design and code software products autonomously. <em>Letting</em> is a much more suitable word than <em>steering</em>, because the alignment of products decisions is highly variable: any language-based instruction set will be grossly inexhaustive and full of interpretative gaps that AI needs to fill.<!-- HTML_TAG_END -->
      </span></div>
                </div>
        </section></div>

    <div class="mt-[4em] md:mt-[5em] max-w-screen-md mx-auto" style="min-height: calc(50vh - 9rem);"><div class="-mx-5 md:-mx-8 bg-white/[0.02] rounded-none md:rounded-3xl p-5 md:p-8 border border-white/5" id="section-notes" data-essay-section="true"><div class="flex justify-start items-start"><h2 class="text-xl md:text-2xl font-semibold text-white">Notes</h2></div>
          <div class="mt-4 space-y-2"><p id="footnote-1" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">1</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->My brother ran a total of about 50km in the months leading up to the marathon. Yes, he&#39;s crazy.<!-- HTML_TAG_END -->
                </p><p id="footnote-2" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">2</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->Competitive pressure is mostly about execution, while customer trust is mostly about product direction.<!-- HTML_TAG_END -->
                </p><p id="footnote-3" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">3</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->Assuming that the user is using an app builder platform, the platform would be wise to use general context about the user to more effectively solve the user&#39;s needs.<!-- HTML_TAG_END -->
                </p><p id="footnote-4" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">4</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->The only chance the user can eliminate information asymmetry about the quality of the software is if there&#39;s an agentic AI solution that QA tests software for the end user. Obviously, this solution has its limits when finances and financial products become involvedâ€”good luck third party QA testing large money transfers, for example. However, I imagine it would be really useful to the end user if there&#39;s the equivalent of an undercover food critic who tests if third party software will work as expected, in the form of an autonomous AI agent.<!-- HTML_TAG_END -->
                </p><p id="footnote-5" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">5</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->&quot;How Ramp engineering operates at hyperspeed with Claude Code&quot;: <a href="https://www.claude.com/customers/ramp">https://www.claude.com/customers/ramp</a>.<!-- HTML_TAG_END -->
                </p><p id="footnote-6" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">6</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->AI can continuously monitor changes to and proof read the latest EULA.<!-- HTML_TAG_END -->
                </p><p id="footnote-7" class="font-serif text-sm md:text-base text-muted-text-grey"><button class="group inline-flex items-center px-1 py-0.5 rounded border-none bg-transparent cursor-pointer hover:bg-gray-700 transition-colors"><span class="text-sm text-muted-text-grey group-hover:text-white transition-colors">[<span class="text-[0.9rem]"> </span>
                      <span class="text-white underline decoration-glacial-blue">7</span><span class="text-[0.32rem]"> </span>
                      <span style=""><svg class="inline text-muted-text-grey group-hover:text-white transition-colors" style="overflow: visible;" width="12px" height="12px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M1.65448 23.0773c4.14279 -0.2844 8.02223 -2.1342 10.85112 -5.1741 2.8289 -3.0399 4.3953 -7.0422 4.3814 -11.19467 0 -0.27665 0.014 -0.93855 0 -1.21441l0 -4.525371" stroke-width="1.5"></path><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" d="M11.4409 6.375 16.8932 0.922705 22.3455 6.375" stroke-width="1.5"></path><!-- HTML_TAG_END --></svg></span>
                      <span class="text-[0.32rem]"> </span>]
                    </span></button>
                  <!-- HTML_TAG_START -->Shoutout Adobe, who have the lethal combination of consistently bad UX and high switching costs.<!-- HTML_TAG_END -->
                </p></div>

          </div></div></div>
</article>
    <div class="flex flex-col pb-11 mt-0.5 md:pt-1 lg:pt-2 md:justify-between md:items-center md:flex-row-reverse"><div class="flex flex-row space-x-2.5 md:space-x-[0.7rem] lg:space-x-3 pb-[1.35rem] md:pb-0 mt-2 md:mt-0"><a href="https://cal.com/kevgug/intro" target="_blank" rel="noreferrer" class="svelte-10zh0tb"><span style=""><svg class="h-4 w-4 lg:h-5 lg:w-5" style="overflow: visible;" width="16px" height="16px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><g id="webcam-video--work-video-meeting-camera-company-conference-office"><path id="Union" fill="currentColor" fill-rule="evenodd" d="M2.571428571428571 3c-0.6819771428571428 0 -1.3360388571428572 0.27092571428571427 -1.8182742857142855 0.7531542857142857C0.27091714285714286 4.235382857142857 0 4.889451428571428 0 5.571428571428571v12.857142857142856c0 0.6819428571428571 0.27091714285714286 1.3361142857142856 0.7531542857142857 1.818342857142857 0.48223542857142854 0.4822285714285714 1.13628 0.7530857142857142 1.8182742857142855 0.7530857142857142H15.428571428571427c0.6819942857142857 0 1.3360457142857143 -0.27085714285714285 1.818342857142857 -0.7530857142857142 0.4822285714285714 -0.4822285714285714 0.7530857142857142 -1.1364 0.7530857142857142 -1.818342857142857v-1.6824857142857144l3.629828571428571 1.608942857142857c0.012685714285714286 0.005657142857142857 0.02571428571428571 0.010971428571428571 0.03874285714285714 0.016114285714285713 1.1456571428571427 0.44211428571428574 2.3314285714285714 -0.4709142857142857 2.3314285714285714 -1.6054285714285714V7.234285714285713c0 -1.11528 -1.1513142857142855 -2.137577142857143 -2.3727428571428573 -1.5882342857142857l-0.0030857142857142854 0.0014399999999999999 -3.6241714285714286 1.6064228571428572V5.571428571428571c0 -0.6819942857142857 -0.27085714285714285 -1.3360457142857143 -0.7530857142857142 -1.8182742857142855C16.764617142857144 3.270925714285714 16.110565714285716 3 15.428571428571427 3H2.571428571428571Z" clip-rule="evenodd" stroke-width="1.7143"></path></g><!-- HTML_TAG_END --></svg></span></a>
    <a href="mailto:kevin@kevingugelmann.com" class="svelte-10zh0tb"><span style=""><svg class="h-4 w-4 lg:h-5 lg:w-5" style="overflow: visible;" width="16px" height="16px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20"><!-- HTML_TAG_START --><g><path fill="currentColor" fill-rule="evenodd" d="M16.88771428571429 0.139706c0.41185714285714287 -0.15035852857142856 0.8578571428571429 -0.180638 1.2864285714285715 -0.08716557142857143 0.4327142857142857 0.09440814285714286 0.8292857142857143 0.311091 1.1424285714285713 0.6242881428571428 0.31314285714285717 0.31319714285714284 0.5298571428571429 0.70974 0.6242857142857143 1.1424857142857143 0.09342857142857143 0.42845714285714287 0.06314285714285715 0.8745714285714286 -0.08714285714285715 1.2863285714285715l-5.105714285714286 15.302785714285715 -0.00028571428571428574 0.001c-0.12442857142857143 0.37642857142857145 -0.3434285714285714 0.7145714285714285 -0.6361428571428571 0.982 -0.29185714285714287 0.26657142857142857 -0.6471571428571429 0.45400000000000007 -1.0319714285714285 0.5441428571428572 -0.3848857142857143 0.09385714285714285 -0.7875857142857142 0.086 -1.1685428571428573 -0.022857142857142857 -0.38061428571428574 -0.10871428571428572 -0.7264 -0.3145714285714286 -1.0035 -0.5971428571428571l-2.739057142857143 -2.7264285714285714 -2.8772285714285717 1.487714285714286c-0.22435714285714287 0.11614285714285714 -0.49331428571428576 0.105 -0.7073571428571428 -0.029142857142857144 -0.21402857142857146 -0.13414285714285715 -0.3413285714285714 -0.37128571428571433 -0.33477142857142855 -0.6238571428571429l0.11795714285714287 -4.5481428571428575 10.064185714285715 -7.3105714285714285c0.399 -0.2898142857142857 0.48757142857142854 -0.8481714285714286 0.1977142857142857 -1.2471285714285716 -0.28985714285714287 -0.39897142857142853 -0.8481571428571428 -0.4874571428571429 -1.2471285714285716 -0.19765714285714286L3.1469 11.554942857142857 0.6758928571428572 9.08392857142857 0.67581 9.083857142857143l-0.00018 -0.0001857142857142857C0.40802571428571427 8.816257142857143 0.21102285714285712 8.486571428571429 0.10231328571428572 8.1242c-0.10793637142857143 -0.3597857142857143 -0.12547257142857143 -0.7405857142857144 -0.05111628571428571 -1.1087 0.07445228571428572 -0.4021571428571428 0.25422585714285717 -0.7774428571428571 0.5210558571428572 -1.0875428571428571 0.26865714285714287 -0.31222857142857147 0.6158185714285714 -0.5470428571428572 1.0056042857142857 -0.6801714285714285l0.0048142857142857145 -0.001642857142857143 0 0.00001428571428571429L16.88771428571429 0.139706Z" clip-rule="evenodd" stroke-width="1.4286"></path></g><!-- HTML_TAG_END --></svg></span></a>
    <a href="https://www.linkedin.com/in/kevingugelmann" target="_blank" rel="noreferrer" class="svelte-10zh0tb"><span style=""><svg class="h-4 w-4 lg:h-5 lg:w-5" style="overflow: visible;" width="16px" height="16px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20"><!-- HTML_TAG_START --><g><path fill="currentColor" fill-rule="evenodd" d="M3.0974142857142857 0.00017065857142857144c-0.0103 -0.00022513514285714287 -0.0206 -0.0002275192857142857 -0.0309 -0.00000715285714285714C2.456514285714286 0.0132176 1.8742999999999999 0.25773142857142856 1.4379 0.6841371428571429 1.0015042857142857 1.1105442857142858 0.7435700000000001 1.6869285714285713 0.71639 2.2964714285714285c-0.0006214285714285715 0.013928571428571429 -0.0008342857142857143 0.027885714285714285 -0.0006385714285714287 0.04184285714285715 0.008741428571428572 0.6228714285714286 0.2564414285714286 1.2185857142857144 0.6919114285714286 1.6640285714285714 0.4354657142857143 0.44544285714285714 1.0254228571428572 0.7065714285714285 1.647937142857143 0.7294 0.014085714285714286 0.0005285714285714286 0.028185714285714286 0.0006285714285714286 0.04228571428571429 0.00030000000000000003 0.6279428571428571 -0.014142857142857145 1.2251285714285713 -0.2747857142857143 1.6624857142857143 -0.7256 0.43735714285714283 -0.45082857142857147 0.6797857142857143 -1.0556142857142856 0.6749 -1.683714285714286l-0.0004714285714285714 -0.021128571428571427c-0.02277142857142857 -0.6092857142857143 -0.27742857142857147 -1.1868714285714286 -0.7119 -1.6146514285714284C4.288442857142857 0.2591671428571429 3.706985714285714 0.01349817142857143 3.0974142857142857 0.00017065857142857144ZM3.083 6.151114285714287c-0.11080000000000001 0 -0.2188142857142857 -0.002442857142857143 -0.3309857142857143 -0.004985714285714286l-0.0026857142857142856 -0.00005714285714285715 -0.00008571428571428571 0h-0.00005714285714285715c-0.1081 -0.002457142857142857 -0.2233142857142857 -0.005057142857142857 -0.3342857142857143 -0.0044428571428571425 -0.21318571428571428 0.0011714285714285715 -0.4813 0.012957142857142858 -0.7369428571428571 0.0945857142857143 -0.28325571428571433 0.09042857142857143 -0.5586985714285715 0.2691428571428572 -0.7431914285714285 0.5856285714285715C0.7655371428571429 7.112157142857143 0.7142857142857143 7.4489 0.7142857142857143 7.784528571428571v10.605328571428572c0 0.33785714285714286 0.05161 0.6801428571428572 0.22858428571428574 0.9722857142857143 0.19459857142857145 0.32142857142857145 0.4831385714285715 0.4898571428571428 0.7679585714285715 0.5682857142857143 0.2551142857142857 0.07028571428571428 0.5222571428571429 0.07257142857142856 0.7282142857142858 0.06842857142857142 0.10688571428571428 -0.002142857142857143 0.1951857142857143 -0.005714285714285714 0.27952857142857146 -0.009000000000000001l0.00007142857142857143 0 0.00008571428571428571 0c0.11617142857142858 -0.004714285714285714 0.22485714285714287 -0.009000000000000001 0.3642714285714286 -0.009000000000000001 0.13952857142857145 0 0.24802857142857143 0.004285714285714286 0.3638857142857143 0.009000000000000001h0.00005714285714285715c0.08385714285714287 0.003285714285714286 0.17158571428571429 0.006857142857142857 0.2777142857142857 0.009000000000000001 0.20442857142857143 0.004142857142857143 0.471 0.002 0.7255857142857143 -0.069 0.28559999999999997 -0.07957142857142857 0.5719428571428572 -0.2502857142857143 0.7636428571428571 -0.5721428571428572 0.17354285714285717 -0.2912857142857143 0.2237 -0.6318571428571429 0.2237 -0.9678571428571429V7.784528571428571c0 -0.33371428571428574 -0.04978571428571429 -0.6688285714285714 -0.2156857142857143 -0.9582 -0.1815857142857143 -0.3167571428571429 -0.45465714285714287 -0.4976142857142857 -0.7385142857142857 -0.5893428571428572 -0.25514285714285717 -0.08245714285714285 -0.5227 -0.09418571428571429 -0.7344571428571429 -0.09535714285714286 -0.11027142857142856 -0.0006142857142857142 -0.22487142857142856 0.0019857142857142855 -0.3324285714285714 0.004428571428571428l-0.0031714285714285716 0.00007142857142857143c-0.11174285714285714 0.0025428571428571427 -0.21952857142857143 0.004985714285714286 -0.3303285714285714 0.004985714285714286Zm8.928085714285714 0.17190000000000003c0.6495428571428571 -0.28997142857142855 1.3610428571428572 -0.4127142857142857 2.069642857142857 -0.35742857142857143 0.6759857142857143 -0.03395714285714286 1.3521285714285716 0.07091428571428571 1.9862714285714287 0.30835714285714283 0.6511428571428571 0.24375714285714287 1.2437142857142858 0.6217571428571429 1.7391428571428573 1.1094428571428572 0.49557142857142855 0.4877 0.8828571428571429 1.0741857142857143 1.137 1.721342857142857 0.2518571428571429 0.6419571428571429 0.36714285714285716 1.3293714285714286 0.3382857142857143 2.0182857142857142v7.2244142857142855c0 0.33385714285714285 -0.04857142857142858 0.6732857142857143 -0.21942857142857142 0.9641428571428572 -0.18957142857142859 0.32271428571428573 -0.4747142857142857 0.49514285714285716 -0.7614285714285715 0.5755714285714285 -0.25457142857142856 0.07142857142857144 -0.5214285714285715 0.07328571428571429 -0.726 0.06928571428571428 -0.10542857142857144 -0.002142857142857143 -0.19314285714285712 -0.005571428571428571 -0.27742857142857147 -0.009000000000000001 -0.11757142857142858 -0.004571428571428572 -0.2282857142857143 -0.009000000000000001 -0.3702857142857143 -0.009000000000000001 -0.14200000000000002 0 -0.2525714285714286 0.004428571428571428 -0.37014285714285716 0.009000000000000001l-0.00008571428571428571 0c-0.08414285714285714 0.0034285714285714284 -0.172 0.006857142857142857 -0.27742857142857147 0.009000000000000001 -0.20442857142857143 0.004 -0.4714285714285715 0.002142857142857143 -0.7258571428571429 -0.06928571428571428 -0.28685714285714287 -0.08042857142857143 -0.5718571428571428 -0.25285714285714284 -0.7615714285714286 -0.5755714285714285 -0.1707142857142857 -0.29085714285714287 -0.21942857142857142 -0.6302857142857142 -0.21942857142857142 -0.9641428571428572V12.691228571428573c0 -0.02028571428571429 0.0008571428571428571 -0.040557142857142854 0.0025714285714285713 -0.06077142857142857 0.016142857142857143 -0.18898571428571428 -0.008571428571428572 -0.3792428571428571 -0.07242857142857144 -0.5578285714285715 -0.06385714285714285 -0.1785857142857143 -0.1655714285714286 -0.3413285714285714 -0.29787142857142856 -0.47718571428571427 -0.13237142857142858 -0.13585714285714287 -0.2924285714285714 -0.24162857142857144 -0.4692857142857143 -0.3101428571428571 -0.17687142857142857 -0.06851428571428572 -0.3664142857142857 -0.09817142857142858 -0.5557571428571428 -0.08694285714285714 -0.020114285714285716 0.0011857142857142858 -0.04027142857142857 0.0015285714285714286 -0.06041428571428572 0.0010142857142857143 -0.1891714285714286 -0.0048000000000000004 -0.3772 0.03062857142857143 -0.5516571428571428 0.10395714285714286 -0.17444285714285715 0.07332857142857144 -0.33131428571428573 0.1828714285714286 -0.4602428571428572 0.3213857142857143 -0.12892857142857142 0.13849999999999998 -0.22697142857142857 0.30282857142857145 -0.2876142857142857 0.4820714285714286 -0.060657142857142854 0.17924285714285715 -0.08254285714285714 0.3693285714285714 -0.06421428571428571 0.5576714285714286 0.002242857142857143 0.023 0.003357142857142857 0.046085714285714285 0.003357142857142857 0.0692v5.6562c0 0.33585714285714285 -0.0501 0.6768571428571428 -0.22435714285714287 0.9685714285714286 -0.1925142857142857 0.3222857142857143 -0.4799 0.49242857142857144 -0.7658142857142858 0.5717142857142857 -0.25497142857142857 0.07071428571428572 -0.5224714285714286 0.07271428571428572 -0.7285714285714286 0.06871428571428571 -0.10624285714285715 -0.002142857142857143 -0.19465714285714286 -0.005714285714285714 -0.2794142857142857 -0.009000000000000001 -0.11787142857142857 -0.004714285714285714 -0.2286857142857143 -0.009000000000000001 -0.37055714285714286 -0.009000000000000001 -0.14200000000000002 0 -0.25264285714285717 0.004428571428571428 -0.3702 0.009000000000000001l-0.00008571428571428571 0c-0.08421428571428571 0.0034285714285714284 -0.1719857142857143 0.006857142857142857 -0.2773857142857143 0.009000000000000001 -0.20442857142857143 0.004 -0.4714285714285715 0.002142857142857143 -0.7258571428571429 -0.06928571428571428 -0.2867571428571429 -0.08042857142857143 -0.5718857142857143 -0.25285714285714284 -0.7614571428571429 -0.5755714285714285 -0.17082857142857144 -0.29085714285714287 -0.21950000000000003 -0.6302857142857142 -0.21950000000000003 -0.9641428571428572V7.7845c0 -0.3792714285714286 0.06837142857142857 -0.7576 0.2898714285714286 -1.0684857142857145 0.23615714285714287 -0.33144285714285715 0.5671 -0.47752857142857147 0.8641000000000001 -0.5369285714285714 0.15764285714285714 -0.03152857142857143 0.35767142857142853 -0.04025714285714286 0.5272 -0.04355714285714286 0.18504285714285715 -0.0036000000000000003 0.39494285714285715 -0.001142857142857143 0.599 0.003357142857142857 0.1914142857142857 0.004214285714285715 0.3708571428571429 0.009857142857142858 0.5291857142857143 0.014857142857142857l0.00005714285714285715 0c0.19882857142857144 0.006257142857142858 0.36434285714285713 0.011471428571428573 0.4784428571428571 0.011471428571428573 0.4886142857142857 0 0.7918714285714286 0.29064285714285715 0.9680285714285715 0.5671857142857143 0.22521428571428573 -0.1588857142857143 0.4661571428571428 -0.2961714285714286 0.7197714285714285 -0.4093857142857143Z" clip-rule="evenodd" stroke-width="1.4286"></path></g><!-- HTML_TAG_END --></svg></span></a>
    <button type="button" class="text-white/80 transition-all ease-outro duration-200 border-none p-0 cursor-pointer bg-transparent hover:text-white hover:ease-intro hover:duration-intro"><span style=""><svg class="h-4 w-4 lg:h-5 lg:w-5" style="overflow: visible;" width="16px" height="16px" fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><!-- HTML_TAG_START --><g id="rss-symbol--wireless-feed-rss-transmit-broadcast"><path id="Union" fill="currentColor" fill-rule="evenodd" d="M2.2788857142857144 2.575782857142857c0 -0.9467794285714285 0.767502857142857 -1.714290857142857 1.7142857142857142 -1.714290857142857 2.5154571428571426 0 5.006194285714286 0.4967125714285714 7.329942857142857 1.4616737142857144 2.3237485714285713 0.9649542857142857 4.434874285714285 2.3791885714285717 6.212999999999999 4.161754285714285 1.7780571428571426 1.7825657142857143 3.188228571428571 3.8985257142857144 4.1502857142857135 6.2269885714285715 0.9620571428571428 2.328445714285714 1.457142857142857 4.823862857142857 1.457142857142857 7.344034285714285 0 0.9466285714285714 -0.7674857142857142 1.7142857142857142 -1.7142857142857142 1.7142857142857142s-1.7142857142857142 -0.7676571428571428 -1.7142857142857142 -1.7142857142857142c0 -2.071028571428571 -0.4068 -4.121691428571428 -1.1972571428571428 -6.0348 -0.7904571428571429 -1.9131085714285714 -1.9489542857142856 -3.6511199999999997 -3.409062857142857 -5.114914285714285 -1.4601085714285713 -1.4637771428571429 -3.1932685714285713 -2.6246914285714285 -5.100411428571428 -3.416657142857143 -1.907142857142857 -0.7919485714285713 -3.9510514285714287 -1.1995028571428572 -6.0150685714285705 -1.1995028571428572 -0.946782857142857 0 -1.7142857142857142 -0.76752 -1.7142857142857142 -1.7142857142857142Zm1.8629485714285712 14.016377142857143c-1.809137142857143 0 -3.2763205714285712 1.4661257142857143 -3.2763205714285712 3.2753828571428567S2.332697142857143 23.142857142857142 4.141834285714285 23.142857142857142s3.276325714285714 -1.4660571428571427 3.276325714285714 -3.275314285714286c0 -1.8092571428571427 -1.4671885714285713 -3.2753828571428567 -3.276325714285714 -3.2753828571428567Zm0.21512571428571425 -8.494937142857141c-0.9467657142857142 0 -1.7142857142857142 0.76752 -1.7142857142857142 1.7142857142857142 0 0.946782857142857 0.76752 1.7142857142857142 1.7142857142857142 1.7142857142857142 1.0684799999999999 0 2.126605714285714 0.21097714285714284 3.1139828571428567 0.621 0.9873942857142858 0.41002285714285713 1.88484 1.01112 2.6409942857142856 1.7691771428571426 0.7561542857142857 0.7580571428571428 1.3562057142857142 1.658245714285714 1.7656628571428572 2.6492742857142857 0.4094742857142857 0.9910971428571428 0.6202799999999999 2.0534399999999997 0.6202799999999999 3.1262399999999997 0 0.9468 0.767502857142857 1.7142857142857142 1.7142857142857142 1.7142857142857142 0.9467657142857142 0 1.7142857142857142 -0.7674857142857142 1.7142857142857142 -1.7142857142857142 0 -1.521942857142857 -0.29902285714285715 -3.029108571428571 -0.8800971428571428 -4.435474285714285 -0.5810742857142857 -1.406382857142857 -1.432885714285714 -2.6845371428571427 -2.507005714285714 -3.7613657142857138 -1.07412 -1.0768285714285712 -2.3495314285714284 -1.9312457142857142 -3.7535142857142856 -2.514274285714286 -1.404 -0.5830114285714285 -2.9089371428571424 -0.8831485714285714 -4.4288742857142855 -0.8831485714285714Z" clip-rule="evenodd" stroke-width="1.7143"></path></g><!-- HTML_TAG_END --></svg></span></button>
    </div>
  <div class="flex flex-col-reverse md:flex-row md:justify-between"><p class="text-muted-text-grey font-light">Product Designer. AI-Native Engineer.<br>
      Copyright Â© 2025 Kevin Gugelmann. All rights reserved.
    </p></div></div>

</div></div>





		<script type="module" data-sveltekit-hydrate="xslgmx">
			import { start } from "../internal/immutable/start-9a263591.js";

			start({
				assets: "",
				env: {},
				target: document.querySelector('[data-sveltekit-hydrate="xslgmx"]').parentNode,
				version: "1761458158854",
				hydrate: {
					node_ids: [0, 2, 3, 6],
					data: [null,null,null,null],
					form: null,
					error: null
				}
			});
		</script>
	<script type="application/json" data-sveltekit-fetched data-url="/essays/index.json">{"status":200,"statusText":"","headers":{},"body":"[\n  {\n    \"slug\": \"accountability-ai-autonomy\",\n    \"title\": \"Selling software without accountability\",\n    \"date\": \"2025-10-24T00:00:00.000Z\",\n    \"publish\": true\n  },\n  {\n    \"slug\": \"manifesto\",\n    \"title\": \"Writing manifesto\",\n    \"date\": \"2025-09-12\",\n    \"publish\": true\n  }\n]"}</script>
	<script type="application/json" data-sveltekit-fetched data-url="/essays/accountability-ai-autonomy.md">{"status":200,"statusText":"","headers":{},"body":"---\ntitle: \"Selling software without accountability\"\ndate: 2025-10-24\npublish: true\n---\n\n## The question\n\nWhen my brother visited me in Chicago to run the 47th Bank of America marathon (on virtually no training!), he raised an interesting question: is **accountability** a good enough reason for why we don't want autonomous AI software engineers? We both agree that full autonomy is not even feasible for at least another two years. The question is more about _when_ AI autonomy becomes feasible, will the need for human accountability get in the way? [1]\n\nIf accountability is necessary for running a profitable business, it follows that humans should always have oversight over every product decision and feature implementation; at most, AI would be a coworker, but could never fully displace humans. Alternatively, accountability is not an important part of building software products. In that scenario, AI can successfully decide what to build and then presumably build it too; software engineers and product designers could very well be replaced by fully autonomous AI.\n\nI view accountability as the idea that someone owns a product decision and accepts whatever consequences may follow, good or bad. If we let AI autonomously decide what to do and how to implement those changes, then we can't say that any person truly _owns_ those decisions. At best, we can say that someone instructed the AI on how to operate at a high level, which led to some product outcome. But that's not a generalizable argumentâ€”sometimes the decision is a direct result of the human's system instructions, while other times the AI has gone rogue. In any case, autonomous AI prevents clear accountability with any person.\n\nSo maybe we can't pinpoint _exactly_ who or what is responsible for certain features, bugs, or errors. Does that negatively affect the bottom line of the software company? To rephrase my brother's question:\n\n> How important is accountability in product teams for building software that sells?\n\nLet's start by looking at our assumptions about why we might need accountability in software teams, meaning each product decision can be traced back to an individual employee.\n\n## Benefits of accountability\n\nI see two main arguments for why firms need accountability to sell software in the pre-AI age.\n\nOne is **competitive pressure** from inside the firm. If a knowledge worker makes exceptionally good work they expect promotion, while for exceptionally poor work they can expect to be fired. Accountability motivates employees to do good work and avoid bad work, because everyone owns their decisions and any subsequent consequences. This motivation produces better software products, which helps create a competitive edge in the software market.\n\nAnother is the **customer trust**. Naturally, the customer wants to know that the software creator won't betray them. If product teams can make decisions without consequences, what stops them from turning their back on the user? With accountability in place, individual employees are directly responsible for lost customers and revenue if they sunset important features or expose publicly-identifiable information (PII).\n\nBoth benefits of accountability help sell software. Are these benefits exclusive to accountability? Let's consider three realistic scenarios in which AI builds software on its own. [2]\n\n## Autonomy scenarios\n\nImagine AI automation moves up the chain, first replacing engineers with designers who direct autonomous AI to make engineering decisions, then replacing designers with product managers who direct autonomous AI to make design decisions. Even the most technical product managers are swapped with C-suite directing increasingly intelligent yet holistic AI systems. Before you know it, even C-suite is offloading all their decisions. At this point, AI is hypothetically making decisions across every facet of the company. There is not a single employee who can be held accountable for any specific feature, bug, or system failure. Who is accountable when something goes wrong?\n\nThere are three plausible scenarios in which all software is designed, coded, and deployed exclusively by AI. The first is exactly as I described: some **small-team company** where the C-suite instructs AI broadly on the company's mission and guiding principles, before kicking their feet up and letting AI make all the product decisions. The second scenario eliminates the company entirely, which is now just a middleman between the user and AI building the softwareâ€”the **user gives AI high-level instructions** on what software they want built for themselves. In the third scenario, the app builder platform uses context about the user to **anticipate the user's needs**, then builds the software accordingly. [3]\n\nIn each scenario, no one can be held fully accountable for how the software turns out, neither the user nor the creator. Since humans can only give AI a finite set of instructions, its decisions will mostly be extrapolations of human rules and intent.\n\nI'd like to test if only accountability can achieve competitive pressure and customer trust, assuming they are both necessary for selling software products. Then we can infer if the absence of accountability, when using autonomous AI systems, would necessarily harm profits.\n\n## Alignment\n\nAlignment is a notable issue with autonomous systems. Even if AI follows all the provided instructions to a T, there will always be edge cases that humans couldn't anticipate or encode preferences for. The system is fully autonomous, so it can't ever ask the human to state or clarify their preferences along the way. AI will need to fill in the gaps with its own preferences, which may or may not be what the software creator would have wanted. There is either human-AI alignment or misalignment: it could go either way.\n\nLet's assume AI makes a decision that is misaligned. The product worsens, users churn, and revenue drops. There is no clear accountability: no individual human made the problematic decision, yet the software suffers. We should ask whether the lack of clear accountability actually hurts the business. Importantly, the market doesn't care _who_ made a bad callâ€”it will respond the same to a misaligned AI as it would to a human making the same misstep. While there is no competitive pressure at an individual level, there is competitive pressure at a company level. If things go sideways, the autonomous system can pick up on that signal and act accordingly.\n\nAutonomous AI might also make an aligned product decision. The product improves and revenue grows. Again, there is no clear accountability: we can't trace the beneficial decision back to an individual. As before, the market doesn't care exactly who created that feature or fixed that bug. So customer trust improves even though accountability is absent. Competitive pressure also acts on the company as a wholeâ€”the AI systemâ€”rather than on individuals that need incentives to work well.\n\nSo there is still competitive pressure even when there's no accountability; the feedback mechanism just acts on a company level rather than an individual level. When aligned, autonomous AI can still maintain or improve customer trust. It's just uncertain whether software changes are aligned (beneficial) or misaligned (harmful), because the human's instructions will have gaps that AI needs to fill on its own.\n\nAll in all, autonomous AI can achieve the same main benefits as accountabilityâ€”competitive pressure and customer trustâ€”just by different, and arguably less certain, means. So autonomous AI can be financially viable, too.\n\n## Blaming users\n\nBut what about our intuition that someone, _anyone_, should be responsible when software goes wrong? We can look back at each scenario for creating software with autonomous AI and point fingers at whoever wrote the AI instructions. For commercial software, we might blame the C-suite. For prompt-based personal software, we might blame the user. And for personal software that anticipates user needs, we might blame the app builder platform. Then again, our previous conclusion hasn't changed: no one can be held fully accountable for how the software turns out. We're just running in circles.\n\nRather than chasing accountability with the software creators, what if we can instead shift the responsibility of good software to the user? This seems backwards and appears to violate every UX principle. The reason is that software markets in the AI age will be highly saturated, due to the rapidly falling barriers for creating softwareâ€”both skills and cost.\n\n> It's not unlikely that a user can choose from thousands of close software substitutes in the near future.\n\nAccountability will disappear as human teams disappear, and the main responsibility of having a good user experience will fall on the users. Because users have many options, they have greater responsibility over which software product they decide to use.\n\n## Blind trust\n\nYou may reasonably say, \"but that doesn't solve the issue of information asymmetry!\" I completely agree. Let's tackle this separate issue.\n\nUsers cannot definitively know ahead of using software if it will work as advertised. However, even today when there are thousands of close substitutes programs for every use case, the overwhelming majority of users blindly accept terms & conditions (T&Cs) and end user license agreements (EULAs). In other words, 99.99% of consumers are already comfortable with blindly trusting the goodwill of software creators to write fine print that is sufficiently aligned with their own goals. After all, EULAs have become longer and more complex over the last decade, and there has only been an increase in software usage. [4]\n\nI see a parallel in blind trust between human-written and AI-written software. I believe that increased AI use in software production will, ceteris paribus, have no effect on software adoption. Most major companies are already writing their software using AI, representing a nonzero percentage of their code. I'm aware some modern companies, like Ramp, even demand their engineers use AI tools to stay ahead of the curve. The software engineering industry has already dialed up the percentage of code produced by AI and the adoption of software has not been affected at all, at least not to my knowledge. If thatâ€™s our analysis at the margin, we also won't expect any difference in adoption between software that is 0%, 50%, 90%, 99.9%, or 100% AI made. 100% percentage represents, of course, our autonomous AI system. Why is that? [5]\n\nI think this effect is for pragmatic reasonsâ€”all customers should take a product at face value to a certain degree. Consider these reasonable user questions:\n\n- Could my phone battery blow up in my hand?\n- Could the tires on my car fall off on the motorway?\n- Could the roof in my lecture hall fall on my head mid-class?\n\nYou could compile a list of valid concerns about using _any_ product, if you put your mind to it. Even something as harmless as a poster on your wall risks you leaving a bad impression on someone important you invite over. The reason we donâ€™t obsess over such low risks is because we simply want the product that offers maximum upside while falling under a risk thresholdâ€”both decided jointly by logical analysis and our gut feeling. The same applies to software products:\n\n> We can only know if software works as expected by using it.\n\nAs for trusting the _legal_ goodwill of software creators, you might think we largely trust T&Cs and EULAs because we assume at least one other user _does_ read T&Cs and EULAs and would publicly flag any harmful fine print. Firstly, this is a precarious assumption: not everyone is an Edward Snowdon who finds and reports harmful software practices, and not every software program has been subjected to an Edward Snowdon. Especially in the AI age when there are thousands of close substitutes. Secondly, I would expect an AI system to be far more reliable, accurate, and faster at detecting harmful fine print. [6]\n\nIn other words, I don't think the majority of users blindly accept EULAs because they are all free riding on some legal nerd who will read them. No, we just all accept that we can't know for sure if software will work as advertised or breach our trust. We're simply practical creatures: we know the fastest way to find out if software works is to use it, and the fastest way to use software is to skip reading the legal paperwork.\n\n## Conclusion\n\nIt turns out, accountability is _not_ vital for selling software. The main benefits of accountabilityâ€”competitive pressure and customer trustâ€”can also be achieved by autonomous AI systems. Competitive pressure is simply shifted from the individual who needs status-based motivation to the company which continuously collects signalsâ€”the company being an autonomous AI system. Customer trust is still feasible, but harder to guarantee when there is no human to validate each decision.\n\nThe customer doesn't need perfect trust in a software product to start using it; it just needs to be _promising enough_. At the same time, if autonomous AI is misaligned with what the creator would have wanted and neglects a user, they will still eventually offboard the software, factoring in switching costs. This is analogous to how users blindly accept T&Cs and EULAs for modern software, proving that the vast majority of users accept information asymmetry to a degree and simply look for sufficiently promising products. [7]\n\nCounter to our intuition, users will be responsible for using bad software in the AI age. The falling cost and skills barrier for building software will flood the commercial software market with thousands of close substitutes. Alternatively, users will build their own hyperpersonalized software with AI app builders, meaning they effectively input their needs and also bear responsibility for any poor UX.\n\nAcross thousands of commercial and personal software products doing generally the same thing, there will be at least one high quality option. The main barriers to picking high quality software will be switching costs, which is already relevant today, and evaluating all substitutes well, which is a future problem. To solve finding the best option, I expect someone will build a QA testing tool for _users_ that tests software on their behalf before they onboard the best one, which is deserving of its own essay.\n\nHuman-AI alignment remains the biggest limiting factor in letting AI design and code software products autonomously. _Letting_ is a much more suitable word than _steering_, because the alignment of products decisions is highly variable: any language-based instruction set will be grossly inexhaustive and full of interpretative gaps that AI needs to fill.\n\n## Notes\n\n[1] My brother ran a total of about 50km in the months leading up to the marathon. Yes, he's crazy.\n\n[2] Competitive pressure is mostly about execution, while customer trust is mostly about product direction.\n\n[3] Assuming that the user is using an app builder platform, the platform would be wise to use general context about the user to more effectively solve the user's needs.\n\n[4] The only chance the user can eliminate information asymmetry about the quality of the software is if there's an agentic AI solution that QA tests software for the end user. Obviously, this solution has its limits when finances and financial products become involvedâ€”good luck third party QA testing large money transfers, for example. However, I imagine it would be really useful to the end user if there's the equivalent of an undercover food critic who tests if third party software will work as expected, in the form of an autonomous AI agent.\n\n[5] \"How Ramp engineering operates at hyperspeed with Claude Code\": https://www.claude.com/customers/ramp.\n\n[6] AI can continuously monitor changes to and proof read the latest EULA.\n\n[7] Shoutout Adobe, who have the lethal combination of consistently bad UX and high switching costs.\n"}</script></div>
  </body>
</html>
